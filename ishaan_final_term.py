# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/IshaanChawla0001/python-project/blob/main/Ishaan_final_term.ipynb
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import datetime
import pandas_datareader as web
import pandas as pd
import seaborn as sb
import math
import matplotlib.pyplot as plt
# %matplotlib inline
from matplotlib import style
import matplotlib as mpl
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
from sklearn import preprocessing

# pandas_datareader library allows us to connect to the website and extract data directly from internet sources in our case we are extracting data from Yahoo Finance API.
start = datetime.datetime(2010, 1, 1)
end = datetime.datetime(2017, 1, 11)
# Read & display the data for Apple
dfa = web.DataReader("AAPL", 'yahoo', start, end)
dfa.tail(10)

close_px = dfa['High']
mavg = close_px.rolling(window=100).mean()
mavg

mpl.rc('figure', figsize=(8, 7))
mpl.__version__

# Adjusting the style of matplotlib
style.use('ggplot')

close_px.plot(label='AAPL')
mavg.plot(label='mavg')
plt.legend()

# Read & display the data for Netflix
dfn = web.DataReader("NFLX", 'yahoo', start, end)
dfn.tail(10)

# Read & display the data for Microsoft
dfm = web.DataReader("MSFT", 'yahoo', start, end)
dfm.tail(10)

# Read & display the data for Google
dfg = web.DataReader("GOOG", 'yahoo', start, end)
dfg.tail(10)

# Pandas reset_index() is a method to reset index of a Data Frame. reset_index() method sets a list of integer ranging from 0 to length of data as index.
dfa.reset_index(inplace=True)
dfn.reset_index(inplace=True)
dfm.reset_index(inplace=True)
dfg.reset_index(inplace=True)
dfa

# Pandas describe() is used to view some basic statistical details like percentile, mean, std etc. of a data frame or a series of numeric values.
dfa.describe()

dfn.describe()
dfm.describe()
dfg.describe()

#    is a measure of association or dependency between two features i.e. how much Y will vary with a variation in X. The correlation method that we will use is the Pearson Correlation.
# this is the correlation of apple stock with its features like high ,low etc.
corra = dfa.corr(method='pearson')
corra

# Not all text is understandable, lets visualize the correlation coefficient.
sb.heatmap(corra,xticklabels=corra.columns, yticklabels=corra.columns,
 cmap='RdBu_r', annot=True, linewidth=0.5)

# here is the Correlation of all stocks with eachother.
dfcomp = web.DataReader(['AAPL', 'GOOG', 'NFLX', 'MSFT'],'yahoo',start=start,end=end)['Adj Close']
dfcomp

retscomp = dfcomp.pct_change()

corr = retscomp.corr()
corr

plt.imshow(corr, cmap='RdBu_r', interpolation='none')
plt.colorbar()
plt.xticks(range(len(corr)), corr.columns)
plt.yticks(range(len(corr)), corr.columns);

# here the graph shows the  
plt.scatter(retscomp.mean(), retscomp.std())
plt.xlabel('Risk')
plt.ylabel('Expected returns')
for label, x, y in zip(retscomp.columns, retscomp.mean(), retscomp.std()):
    plt.annotate( 
        label, 
        xy = (x, y), xytext = (20, -20),
        textcoords = 'offset points', ha = 'right', va = 'bottom',
        bbox = dict(boxstyle = 'round,pad=0.5', fc = 'yellow', alpha = 0.5),
        arrowprops = dict(arrowstyle = '->', connectionstyle = 'arc3,rad=0'))

# Plot Open vs Close
dfa[['Open','Close']].head(20).plot(kind='bar',figsize=(16,8))
plt.grid(which='major', linestyle='-', linewidth='0.5', color='green')
plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')
plt.show()

"""# **For this model the confusion matrix cannot be designed as for the confusion matrix the values should be distinct and non continious, but in my model the accuracy is so high that the values are pretty similar and continious**

# **Model Training and Testing**
"""

# Date format is DateTime and it will throw error while training so I have created seperate month, year and date entities
# Apple 
dfa['Year']=dfa['Date'].dt.year
dfa['Month']=dfa['Date'].dt.year
dfa['Day']=dfa['Date'].dt.year
dfa['Year']

# Netflix 
dfn['Year']=dfn['Date'].dt.year
dfn['Month']=dfn['Date'].dt.year
dfn['Day']=dfn['Date'].dt.year
# Microsoft 
dfm['Year']=dfm['Date'].dt.year
dfm['Month']=dfm['Date'].dt.year
dfm['Day']=dfm['Date'].dt.year
# Google 
dfg['Year']=dfg['Date'].dt.year
dfg['Month']=dfg['Date'].dt.year
dfg['Day']=dfg['Date'].dt.year

dfreg = dfa.loc[:,['Adj Close','Volume']]
dfreg['HL_PCT'] = (dfa['High'] - dfa['Low']) / dfa['Close'] * 100.0
dfreg['PCT_change'] = (dfa['Close'] - dfa['Open']) / dfa['Open'] * 100.0
dfreg

aapl_df=dfa[['Day','Month','Year','High','Open','Low','Close']]
aapl_df.head(10)

# for other stocks
nflx_df=dfn[['Day','Month','Year','High','Open','Low','Close']]
nflx_df.head(10)
msft_df=dfm[['Day','Month','Year','High','Open','Low','Close']]
msft_df.head(10)
goog_df=dfg[['Day','Month','Year','High','Open','Low','Close']]
goog_df.head(10)

#separate Independent and dependent variable
X = aapl_df.iloc[:,3:6]
Y= aapl_df.iloc[:, 6]
print(X.shape)
print(Y.shape)

"""# **Splitting the dataset into train and test**"""

x_train,x_test,y_train,y_test= train_test_split(X,Y,train_size=0.8,test_size=0.2, random_state=101)

print(x_train.shape) 
print(x_test.shape)  
print(y_train.shape)
print(y_test.shape) 
#y_test to be evaluated with y_pred for Diff models

"""# **Linear Regression Model Training and Testing**"""

regressor = LinearRegression()
regressor.fit(x_train, y_train)
y_pred=regressor.predict(x_test)
df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})
df

"""# **Plotting the data of linear regression (Actual VS Pred)**"""

plot_df=pd.DataFrame({'Actual':y_test,'Pred':y_pred})
plot_df.head(20).plot(kind='bar',figsize=(16,8))
plt.grid(which='major', linestyle='-', linewidth='0.5', color='green')
plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')
plt.show()

"""# **Function cost & Accuracy Linear regression**"""

# Commented out IPython magic to ensure Python compatibility.
# The mean squared error # cost function
print('Mean squared error: %.2f'
#       % mean_squared_error(y_test,y_pred))

# Commented out IPython magic to ensure Python compatibility.
# The coefficient of determination: 1 is perfect prediction r2score & accuracy score.
print('Coefficient of determination: %.2f'
#       % r2_score(y_test,y_pred))

"""# **Model comparison with other model**

Linear Model Cross-Validation

---

Basically Cross Validation is a technique using which Model is evaluated on the dataset on which it is not trained i.e. it can be a test data or can be another set as per availability or feasibility.

# **KNN Cross-Validation**
"""

from sklearn import model_selection
from sklearn.model_selection import KFold
kfold = model_selection.KFold(n_splits=20, random_state=100)
results_kfold = model_selection.cross_val_score(regressor, x_test, y_test.astype('int'), cv=kfold)
print("Accuracy: ", results_kfold.mean()*100)

"""# **KNN model training & testing**"""

from sklearn.neighbors import KNeighborsRegressor
knn_regressor=KNeighborsRegressor(n_neighbors = 5)
knn_model=knn_regressor.fit(x_train,y_train)
y_knn_pred=knn_model.predict(x_test)

knn_kfold = model_selection.KFold(n_splits=20, random_state=100)
results_kfold = model_selection.cross_val_score(knn_model, x_test, y_test.astype('int'), cv=knn_kfold)
print(results_kfold.mean()*100)

df = pd.DataFrame({'Actual': y_test, 'Predicted': y_knn_pred})
df

plot_knn_df=pd.DataFrame({'Actual':y_test,'Pred':y_knn_pred})
plot_knn_df.head(20).plot(kind='bar',figsize=(16,8))
plt.grid(which='major', linestyle='-', linewidth='0.5', color='green')
plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')
plt.show()

"""# **Function cost KNN**"""

knn_mse=math.sqrt(mean_squared_error(y_test,y_knn_pred))
print(knn_mse)